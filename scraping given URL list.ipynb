{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e38dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import chromedriver_autoinstaller\n",
    "import tldextract\n",
    "import random\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "#from skimpy import skim\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "656e9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium headless chrome driver\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"user-agent=INDIAN_UNIVERSITIES\")\n",
    "\n",
    "# block file downloads\n",
    "prefs = {\"download_restrictions\": 3}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "# Initialize the Selenium WebDriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# driver.set_page_load_timeout(60)  # 60 seconds timeout for page load only if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a79c26fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain name from URL\n",
    "def extract_domain(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    # This will combine the domain and the top-level domain (TLD)\n",
    "    return f\"{extracted.domain}.{extracted.suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1df62909",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"homepage_urls_usa.pkl\", \"rb\") as file:\n",
    "     all_urls = pickle.load(file)[22021:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a7ce025-0f60-477e-8f0b-35b2b27f64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"homepage_urls_usa2.txt\", \"rb\") as file:\n",
    "    all_urls = [url.decode('utf-8') if isinstance(url, bytes) else url for url in all_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33efe00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 226/226 [07:20<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for URL \n",
      ": InvalidArgumentException()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_page_data(url):\n",
    "    \"\"\"Extracts page title and cleaned text from the given URL.\"\"\"\n",
    "    try:\n",
    "        # WebDriver load page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Create BeautifulSoup object and specify the parser\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract title of the page\n",
    "        title = soup.title.string if soup.title else \"No title\"\n",
    "\n",
    "        # Extract all text from the page\n",
    "        raw_text = soup.get_text()\n",
    "        # Clean the text by replacing multiple white spaces and line breaks with single space\n",
    "        cleaned_text = \" \".join(raw_text.split())\n",
    "\n",
    "        return title, cleaned_text\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout exception for URL: {url}\")\n",
    "        return (\"No Data\", \"No Data\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error for URL {url}: {repr(e)}\")\n",
    "        return (\"No Data\", \"No Data\")\n",
    "\n",
    "# Initialize an empty list to hold the data\n",
    "data = []\n",
    "\n",
    "# For each URL in the list of URLs\n",
    "for url in tqdm(all_urls):\n",
    "    title, cleaned_text = extract_page_data(url)\n",
    "    website = extract_domain(url)\n",
    "    data.append([website, url, title, cleaned_text])\n",
    "\n",
    "# Also save as a Parquet file as it takes less space on server\n",
    "# df.to_parquet(\"scraped_homepage_urls_india.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0b3b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"website\", \"url\", \"title\", \"text\"])\n",
    "\n",
    "# Save as CSV file\n",
    "df.to_csv(\n",
    "    \"usa-bis.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe450eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
