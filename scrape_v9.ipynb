{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd20477d-88fc-4584-b120-29c0353433d5",
   "metadata": {},
   "source": [
    "# Scrape University Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809f04c-2447-442a-9b85-e8ae373c0e89",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a191f-c676-49e5-bd5c-ca6c8af991ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import chromedriver_autoinstaller\n",
    "import tldextract\n",
    "import random\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from skimpy import skim\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee6102-9a91-4d15-9165-ab2ba25a1cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selenium headless chrome driver\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_argument(\"user-agent=INDIAN_UNIVERSITIES\")\n",
    "\n",
    "# block file downloads\n",
    "prefs = {\"download_restrictions\": 3}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "# Initialize the Selenium WebDriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# driver.set_page_load_timeout(60)  # 60 seconds timeout for page load only if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4c6ba-09df-4f39-aea0-0c94c884c0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_webpage_extensions = [\n",
    "    \".pdf\",\n",
    "    \".doc\",\n",
    "    \".docx\",\n",
    "    \".xls\",\n",
    "    \".xlsx\",\n",
    "    \".ppt\",\n",
    "    \".pptx\",  # Documents\n",
    "    \".png\",\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".gif\",\n",
    "    \".bmp\",\n",
    "    \".tiff\",\n",
    "    \".svg\",  # Images\n",
    "    \".mp3\",\n",
    "    \".wav\",\n",
    "    \".ogg\",\n",
    "    \".mp4\",\n",
    "    \".avi\",\n",
    "    \".mkv\",\n",
    "    \".flv\",  # Audio & Video\n",
    "    \".zip\",\n",
    "    \".rar\",\n",
    "    \".tar\",\n",
    "    \".gz\",\n",
    "    \".7z\",  # Archive\n",
    "    \".exe\",\n",
    "    \".dmg\",\n",
    "    \".iso\",\n",
    "    \".bin\",  # Executable & Disk Images\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33ad20-3a40-4d43-83a9-78376202b106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract domain name from URL\n",
    "def extract_domain(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    # This will combine the domain and the top-level domain (TLD)\n",
    "    return f\"{extracted.domain}.{extracted.suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49abd2-3e10-4eee-bab1-a7554bd087be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to get all links within the website. Manual checks are needed later on misses.\n",
    "\n",
    "\n",
    "def extract_links_from_page(driver, url, base_url):\n",
    "    \"\"\"\n",
    "    Extracts all the links from a given webpage using Selenium, ignoring specific file types and external links.\n",
    "\n",
    "    Args:\n",
    "    - driver: A Selenium WebDriver instance.\n",
    "    - url: The URL of the webpage to extract links from.\n",
    "    - base_url: The base URL of the website to ensure links are internal.\n",
    "\n",
    "    Returns:\n",
    "    - A set of links from the webpage.\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    links_on_page = set()\n",
    "    elements = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "    for elem in elements:\n",
    "        href = elem.get_attribute(\"href\")\n",
    "        if (\n",
    "            href\n",
    "            and extract_domain(base_url) in href\n",
    "            and not href.startswith(\"mailto:\")\n",
    "            and not any(ext in href for ext in non_webpage_extensions)\n",
    "        ):\n",
    "            links_on_page.add(href)\n",
    "    return links_on_page\n",
    "\n",
    "\n",
    "def get_all_website_links(base_url):\n",
    "    visited_links = set()\n",
    "    links_to_visit = [(base_url, 0)]  # Queue initialized with base_url and depth 0\n",
    "    all_links = set()\n",
    "\n",
    "    while links_to_visit:\n",
    "        current_link, depth = links_to_visit.pop(0)  # Dequeue from the front\n",
    "        if current_link not in visited_links:\n",
    "            visited_links.add(current_link)\n",
    "            try:\n",
    "                links_on_current_page = extract_links_from_page(\n",
    "                    driver, current_link, base_url\n",
    "                )\n",
    "                all_links.update(links_on_current_page)\n",
    "                # Add new links with incremented depth\n",
    "                links_to_visit.extend(\n",
    "                    [\n",
    "                        (link, depth + 1)\n",
    "                        for link in links_on_current_page\n",
    "                        if extract_domain(base_url) in link\n",
    "                        and link not in visited_links\n",
    "                    ]\n",
    "                )\n",
    "                print(\n",
    "                    f\"Total links found so far: {len(all_links)}. Current depth: {depth}\"\n",
    "                )\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout while processing {current_link}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error while processing {current_link}. Exception: {e}\")\n",
    "                continue\n",
    "\n",
    "    return list(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19d461-10a0-40cf-8ff1-4c2342a1e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_data(url):\n",
    "    \"\"\"Extracts page title and cleaned text from the given URL.\"\"\"\n",
    "    try:\n",
    "        # WebDriver load page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Create BeautifulSoup object and specify the parser\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract title of the page\n",
    "        title = soup.title.string if soup.title else \"No title\"\n",
    "\n",
    "        # Extract all text from the page\n",
    "        raw_text = soup.get_text()\n",
    "        # Clean the text by replacing multiple white spaces and line breaks with single space\n",
    "        cleaned_text = \" \".join(raw_text.split())\n",
    "\n",
    "        return title, cleaned_text\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout exception for URL: {url}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error for URL {url}: {repr(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93079f7-9905-433c-be14-fe4b23074e77",
   "metadata": {},
   "source": [
    "## Phase 1: Fetch All URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef360e2-28a6-49a1-b20e-c933f8393da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_list = list(\n",
    "    pd.read_csv(\"../data/ugc/india_websites_cleaned.csv\")[\"website\"].dropna()\n",
    ")\n",
    "len(website_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92a56e-0128-40c2-a17c-1b7ee3f0217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store all URLs from all websites\n",
    "all_urls = []\n",
    "\n",
    "# Loop through each website\n",
    "for website in tqdm(website_list):\n",
    "    # print(f\"Processing: {website}\")\n",
    "    urls_from_website = get_all_website_links(website)\n",
    "    all_urls.extend(\n",
    "        urls_from_website\n",
    "    )  # Add URLs from the current website to the master list\n",
    "\n",
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c1fd1-9ebe-42f4-aa1e-0b05c0076ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of all URLs before we continue\n",
    "with open(\"../data/all_urls_v1.pkl\", \"wb\") as file:\n",
    "    pickle.dump(all_urls, file)\n",
    "\n",
    "# # read list of urls if needed\n",
    "# with open(\"../data/all_urls_v1.pkl\", \"rb\") as file:\n",
    "#     all_urls = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55aaf8-79b1-4019-adfd-0bb240484166",
   "metadata": {},
   "source": [
    "## Phase 2: Fetch Data from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fbce2-a224-46ed-9e3a-ae2a26cff396",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(210420)  # set a seed for consistency\n",
    "\n",
    "random.shuffle(\n",
    "    all_urls\n",
    ")  # randomly shuffle all collected URLs to avoid pinging the same website at short intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cfcdc-7221-4bbb-9d9e-6ea9c546e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the data\n",
    "data = []\n",
    "\n",
    "# For each URL in the list of URLs\n",
    "for url in tqdm(all_urls):\n",
    "    title, cleaned_text = extract_page_data(url)\n",
    "    # Append the page URL, page title and cleaned text to the data list\n",
    "    data.append([url, title, cleaned_text])\n",
    "\n",
    "# Convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"URL\", \"Title\", \"Text\"])\n",
    "\n",
    "skim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c84367-46cb-4970-9184-d24b8776b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV file\n",
    "df.to_csv(\n",
    "    \"../data/scraped_sites_v1.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "# Also save as a Parquet file as it takes less space on server\n",
    "df.to_parquet(\"../data/scraped_sites_v1.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27fdbc-0b31-42cb-895e-99c154a864bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()  # Close the WebDriver instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754c9f2-ff3b-46e2-91bd-f92bd573fa43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
